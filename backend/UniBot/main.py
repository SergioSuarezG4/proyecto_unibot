# ==========================================================
# main.py â€” Chatbot con GPT + RAG + FAISS (2025)
# ==========================================================

from flask import Flask, request, jsonify
import os
import random

# ==========================================================
# ğŸ”§ Inicializar Flask ANTES de usar CORS
# ==========================================================
from flask_cors import CORS

app = Flask(__name__)
CORS(app)

# ==========================================================
# ğŸ”§ Cargar variables de entorno ANTES de usar OpenAI
# ==========================================================
from dotenv import load_dotenv
dotenv_path = os.path.join(os.path.dirname(__file__), ".env")
load_dotenv(dotenv_path, override=True)

print("API KEY DETECTADA:", os.getenv("OPENAI_API_KEY"))

# ==========================================================
# ğŸ”‘ Cliente OpenAI (SDK oficial 2025)
# ==========================================================
from openai import OpenAI
client = OpenAI()

# Modelos previos (clusters del usuario)
from chatbot.data import training_data
from chatbot.model import build_and_train_model, load_model, predict_cluster

# Procesamiento de documentos
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import PyPDFLoader, TextLoader, Docx2txtLoader
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS

VECTOR_PATH = "vector_db"
DOCS_PATH = "docs"

# ==========================================================
# ğŸ“„ Cargar y vectorizar documentos estÃ¡ticos
# ==========================================================
def cargar_docs_estaticos():
    all_docs = []
    for file in os.listdir(DOCS_PATH):
        path = os.path.join(DOCS_PATH, file)
        ext = file.split(".")[-1].lower()

        if ext == "pdf":
            loader = PyPDFLoader(path)
        elif ext == "txt":
            loader = TextLoader(path)
        elif ext == "docx":
            loader = Docx2txtLoader(path)
        else:
            continue

        all_docs.extend(loader.load())

    splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=100)
    chunks = splitter.split_documents(all_docs)

    embeddings = OpenAIEmbeddings()
    vector_db = FAISS.from_documents(chunks, embeddings)
    vector_db.save_local(VECTOR_PATH)
    return vector_db

# ==========================================================
# ğŸš€ Cargar vector DB al iniciar servidor
# ==========================================================
if os.path.exists(VECTOR_PATH):
    embeddings = OpenAIEmbeddings()
    vector_db = FAISS.load_local(VECTOR_PATH, embeddings, allow_dangerous_deserialization=True)
else:
    vector_db = cargar_docs_estaticos()

# ==========================================================
# ğŸ¤– Modelo de Clusters del Usuario
# ==========================================================
model, vectorizer = load_model()
if model is None:
    model, vectorizer = build_and_train_model(training_data, n_clusters=6)

RESPUESTAS = {
    0: ["Â¡Hola! ğŸ˜Š Â¿CÃ³mo estÃ¡s?", "Â¡QuÃ© gusto saludarte!", "Â¿En quÃ© puedo ayudarte hoy?"],
    1: ["Hasta luego ğŸ‘‹", "Nos vemos pronto.", "Â¡CuÃ­date! ğŸ˜Š"],
    2: ["Soy un asistente virtual creado para ayudarte ğŸ’»", "PregÃºntame lo que quieras ğŸ˜‰"],
    3: ["Â¡Claro! Â¿En quÃ© puedo ayudarte?", "CuÃ©ntame tu problema ğŸ¤–"],
    4: ["Â¡Gracias a ti! â¤ï¸", "Me alegra ser de ayuda ğŸ˜„"],
    5: ["Lamento eso ğŸ˜”, puedo intentarlo nuevamente.", "Parece que algo no saliÃ³ bien ğŸ˜…"],
}

# Listas para respuestas rÃ¡pidas
SALUDOS = ["hola", "buenas", "hey", "que mÃ¡s", "saludos", "hello"]
DESPEDIDAS = ["chao", "adios", "hasta luego"]
AGRADECIMIENTOS = ["gracias", "muchas gracias", "te lo agradezco"]


# ==========================================================
# ğŸŒ RUTAS FLASK
# ==========================================================
@app.route("/chat", methods=["POST"])
def chat():
    user_text = request.form.get("message", "").strip().lower()

    if not user_text:
        return jsonify({"response": "Por favor escribe algo ğŸ˜…"})

    # ==========================================================
    # ğŸ§  1ï¸âƒ£ Respuestas rÃ¡pidas (sin pasar por RAG)
    # ==========================================================
    if user_text in SALUDOS:
        return jsonify({"response": random.choice(RESPUESTAS[0])})

    if user_text in DESPEDIDAS:
        return jsonify({"response": random.choice(RESPUESTAS[1])})

    if user_text in AGRADECIMIENTOS:
        return jsonify({"response": random.choice(RESPUESTAS[4])})

    # ==========================================================
    # ğŸ” 2ï¸âƒ£ RAG: BÃºsqueda de informaciÃ³n en documentos
    # ==========================================================
    try:
        retriever = vector_db.as_retriever(search_kwargs={"k": 3})
        docs = retriever.invoke(user_text)

        # Si no hay documentos relevantes â†’ usar clusters
        if not docs or len(docs) == 0:
            cluster = predict_cluster(model, vectorizer, user_text)
            response = random.choice(
                RESPUESTAS.get(cluster, ["No estoy seguro ğŸ˜… pero intento ayudarte."])
            )
            return jsonify({"response": response})

        contexto = "\n\n".join([d.page_content for d in docs])

        prompt = f"""
Eres un asistente universitario sobre informaciÃ³n verificada de la Universidad de Caldas, Manizales, Colombia (2025).
Responde ÃšNICAMENTE usando la informaciÃ³n del contexto.
Si la informaciÃ³n no estÃ¡ en el contexto, responde:
"No tengo informaciÃ³n sobre eso".

--- CONTEXTO ---
{contexto}
----------------

Pregunta del usuario:
{user_text}
"""

        ai_response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "Eres un asistente amable, Ãºtil y preciso."},
                {"role": "user", "content": prompt}
            ]
        )

        return jsonify({"response": ai_response.choices[0].message.content})

    except Exception as e:
        print("âš  Error en RAG:", e)

    # ==========================================================
    # ğŸ”„ 3ï¸âƒ£ Fallback final: Clusters
    # ==========================================================
    cluster = predict_cluster(model, vectorizer, user_text)
    response = random.choice(
        RESPUESTAS.get(cluster, ["No estoy seguro ğŸ˜… pero puedo intentarlo otra vez."])
    )
    return jsonify({"response": response})


# ==========================================================
# ğŸš€ Ejecutar servidor
# ==========================================================
if __name__ == "__main__":
    app.run(host="0.0.0.0", port=5000, debug=True)
